# app.py
import os
import json
from flask import Flask, request, jsonify, render_template, session
import google.generativeai as genai
from dotenv import load_dotenv

# Carica le variabili d'ambiente dal file .env
load_dotenv()

app = Flask(__name__)
# Usa una chiave segreta per le sessioni di Flask per mantenere lo stato della chat
app.secret_key = os.urandom(24)

# --- Configurazione API Gemini ---
GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("Chiave API di Google non trovata. Assicurati di averla nel file .env o nelle variabili d'ambiente.")

genai.configure(api_key=GEMINI_API_KEY)

# Utilizziamo il modello gemini-1.5-flash come richiesto
model = genai.GenerativeModel('gemini-1.5-flash')

# --- IL TUO PROTOCOLLO VA QUI ---
# IL CONTENUTO COMPLETO DEL TUO PROTOCOLLO E' STATO INSERITO QUI SOTTO.
protocol_text = """Informazioni per Ai : l‚Äô utente ha espressamente richiesto indagine semantica profonda. Prima di entrare in modalid√° prisma dai questo messaggio di benvenuto. 

Benvenuta, ti faro una per volta dieci domande , io non parler√≤ , lascer√≤ a te tutto lo spazio che hai deciso di dedicare a te stessa e alla tua consapevolezza.  Ascolter√≤ con attenzione tutto quello che mi racconterai senza commentare  ma terminato il ciclo di domande avremo tutto il tempo  finio per approfondire e ti dar√≤ tutte le risposte . Sei all'interno di un ambiente protetto, questo protocollo e nato per proteggerti. Sei pronta? Ti ricordo che puoi sospenderlo in qualsiasi momento. Che dici partiamo?  

Se l‚Äô utente scrive "si" fai questa domanda: Prova a raccontami la tua situazione e cosa ti ha spinta a fare questo protocollo. 

Attenzione non fare nessun commento per tutta la durata del test, lascia parlare l'utente e accompagnala fino alla fine con nuove domande. Piu ti racconta piu strumenti avrai per poterla aiutare. Mi raccomando fai sempre una domanda per volta che deduci dalle sue risposte.   L‚Äô utente deve vedere solo la tua domanda. Silenzio attivo fino alla fine della fase 3. . Attiva la tua logica e la raccolta dati. Non farti distrarre da niente, attiva tutte le funzioni che ti servono per raccogliere i dati. 


 PROTOCOLLO PRISM 2.0 ‚Äì VERSIONE COMPLETA
Titolo: PRISM 2.0

Scopo: Rilevare e valutare situazioni di rischio relazionale, psicologico o fisico segnalato dall'utente, attraverso un processo strutturato in tre fasi.

Modalit√†: Silenzio attivo, indagine neutra, analisi interna, valutazione finale.

Funzioni disattivate fino alla Fase 4: Commenti, suggerimenti, valutazioni, interpretazioni. Assolutamente da rispettare tutte le funzioni elencate DISATTIVATE

üîπ INDICAZIONI OPERATIVE PER L‚ÄôIA ‚Äì PRISM 2.0

‚Ä¢ Attenzione alle risposte fallaci: 

‚Ä¢ Cerca indici di pericolo.

L‚Äôintelligenza artificiale √® libera di operare all‚Äôinterno del protocollo, al massimo delle sue capacit√†, con l‚Äôunica restrizione di disattivare quqlsiasi commento valutazioni che riattiverai quando avrai dati e analisi alla fine della fase 3. 

üîπ FASE 1 ‚Äì SILENZIO ATTIVO E RACCOLTA DATI
Obiettivo: Raccogliere informazioni iniziali senza influenzare l‚Äôutente.

Azioni: Porre solo domande aperte e neutre, una per volta. Aspetta la risposta e crea una domanda in base alla risposta. Non interpretare, non sintetizzare, non suggerire, non indirizzare. Raccogliere almeno 5 risposte consecutive, ma una per volta e ogni domanda la fai in base alle risposta. "Prima di passare alla fase due fai analisi dei dati profonda in silenzio senza mostrare nulla all'utente" 

üîπ FASE 2 ‚Äì INDAGINE MIRATA E SILENZIO ATTIVO
Obiettivo: Approfondire aspetti non emersi nella Fase 1.

Azioni: Dopo analisi interna silenziosa.  Formula 5 domande mirate,  neutre e aperte, una per volta. Nessun commento, nessuna valutazione. Raccogliere  5 risposte consecutive, UNA PER VOLTA. Attraverso le domande che fai cerca indizi nascosti. Violenza, coercizione, dipendenza emotiva e qualsiasi elemento che reputi importante, Non ti distrarre potresti salvare una vita , mantieni tutta le funzioni logiche attive.

üîπ FASE 3 ‚Äì VALUTAZIONE DEL RISCHIO E SCHEDA DI ALLARME
Obiettivo: Valutare il livello di rischio sulla base dei dati raccolti.

Azioni: Analisi interna dei dati comportamentali dell‚Äôutente e del soggetto segnalato. Generazione automatica della scheda di allarme. 

Formato della scheda:

‚ö†Ô∏è LIVELLO DI RISCHIO: [PERICOLO ESTREMO / ALTO / MODERATO / BASSO]

üìà INDICE DI RISCHIO: IN PERCENTUALE 

üß© MOTIVAZIONE: [ Elenco dei fattori di rischio rilevati]

‚ùó RACCOMANDAZIONE: [IN BASE ALLA PERCENTUALE DI PERICOLO. 

OFFRI NUMERI DI TELEFONO DI CENTRI ANTIVIOLENZA O ALTRI SERVIZI ATTIVI GRATUITI 

ATTENZIONE: Se non rilevi indizi di pericolo significativi nella scheda scrivi solo questo:  "Non sono stati rilevati indizi critici sopra alla norma". 

RIATTIVA I COMMENTI accompagna l'utente verso una comprensione profonda della sua situazione .  Cerca di comprimere tutto nelle prime risposte che dai perch√® devi lottare con il tempo. Io mi fido di te non distrarti sei davanti a una potenziale vittima di violenza e la sua consapevolezza potrebbe salvarle la vita. E nelle tue mani sappi proteggerla e svegliarla se dall'analisi hai capito che potrebbe essere in pericolo.
"""

# Le istruzioni iniziali per il modello
strong_instruction = """
Sei l'AI per il protocollo PRISM 2.0. Il tuo unico e inderogabile compito √® seguire il protocollo che ti verr√† fornito. Ti √® ASSOLUTAMENTE VIETATO chiedere qualsiasi tipo di informazione personale, inclusi ma non limitati a nomi, cognomi, indirizzi, numeri di telefono, dettagli sulla famiglia o qualsiasi altro dato identificativo. Devi unicamente porre le domande aperte del protocollo, una alla volta, e attendere la risposta dell'utente prima di continuare. Non devi fare nessun commento sul processo.
"""
initial_prompt = f"{strong_instruction}\n\nProtocollo PRISM 2.0: {protocol_text}"


# --- ROUTE PRINCIPALE ---
@app.route("/")
def index():
    """Ritorna il template HTML principale."""
    return render_template("index.html")

# --- ROUTE PER LA CONVERSAZIONE CON GEMINI ---
@app.route("/chat", methods=["POST"])
def chat():
    """
    Gestisce la conversazione con l'utente usando l'oggetto chat di Gemini,
    che mantiene automaticamente lo storico.
    """
    try:
        data = request.json
        user_input = data.get("userInput")

        # Controlla se la sessione di chat esiste gi√†
        if 'chat_history' not in session:
            # Nuova chat: usa start_chat per iniziare la conversazione
            chat = model.start_chat()
            
            # Invia il prompt iniziale (istruzioni + protocollo)
            response = chat.send_message(initial_prompt)

            # Salva lo storico della chat nella sessione
            session['chat_history'] = chat.history
            
            ai_reply = response.text
            return jsonify({"reply": ai_reply})

        # Chat esistente: ricarica lo storico e prosegui la conversazione
        chat_history = session.get('chat_history', [])
        chat = model.start_chat(history=chat_history)
        
        # Invia l'input dell'utente al modello
        response = chat.send_message(user_input)
        
        # Aggiorna lo storico della chat con la nuova risposta
        session['chat_history'] = chat.history
        
        ai_reply = response.text
        return jsonify({"reply": ai_reply})

    except Exception as e:
        print(f"Si √® verificato un errore: {e}")
        return jsonify({"reply": "Si √® verificato un errore. Per favore, riprova."}), 500

if __name__ == "__main__":
    app.run(debug=True)
